{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interstate-domain",
   "metadata": {},
   "source": [
    "# Data Prepration\n",
    "\n",
    "\n",
    "\n",
    "In this tutorial, we mainly work with the data collected from \"parenting\" and \"feminisim\" forums from Mumsnet and Reddit platforms.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### How to get the data from Reddit?\n",
    "\n",
    "\n",
    "1) Reddit API wrapper, such as PRAW\n",
    "\n",
    "2) GoogleBigQuery \n",
    "\n",
    "For how to use these apptoaches to get the data from Reddit, check the ./extra/GoogleBigQuery.txt and ./extra/RedditCrawler.ipynb\n",
    "\n",
    "### How to get the data from Mumsnet?\n",
    "Mumsnet is not a public website, so, in order to collect its data you need their permision. And then you can write a crawler to collect the data. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We need to take a few steps to convert the raw data (forum text data) to the one that we can use for our salient social identity detection model (LIWC feature vectors):\n",
    "\n",
    "1- Cleaning\n",
    "\n",
    "2- Preparing the data for extracting the LIWC categorise\n",
    "\n",
    "3- Processing for LIWC features\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "First, read the data from CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "# path to the csv files\n",
    "path = './preprocessing/sample_raw_data/'\n",
    "file_name = 'sample'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# reading reddit file\n",
    "sample = pd.read_csv(path + file_name + '.csv')\n",
    "\n",
    "print(list(sample))\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-tactics",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "These are the steps I used to preprocesses raw csv files to remove copy artifacts, and prepare them for importing into the database. These steps are particularly relevant to the Mumsnet dataset and Reddit dataset, but they may be helpful for data collected from other platforms too.\n",
    "\n",
    "###### Dropping unwanted posts\n",
    "\n",
    "Sometimes a post is deleted by the moderator of forum or by user herself. \n",
    "\n",
    "\n",
    "In case of mumsnet, these posts could be detected by matching with patterns such as \"Message deleted\", \n",
    "\"Message deleted by Mumsnet\", \"Message withdrawn\", \"Message withdrawn at poster's request\". For reddit data, there are patterns like \"[deleted]\", \"[removed]\", \"[deleted by user]\". \n",
    "We also drop the posts which belong to users who are no longer in the platform, by matching with patterns like\n",
    "\"[deleted]\", and \"[removed]\". \n",
    "\n",
    "There are also some posts generated with bots and contains patterns like  \"I'm a bot...\", . This is particularly relevant to reddit data but it may be helpful for other data too.\n",
    "\n",
    "\n",
    "\n",
    "For this, we design some regular experessions to capture these patterns inside text and then drop them from our data:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "\n",
    "#regelar expressions to find the matched patterns\n",
    "regex_1 = \"message deleted\"\n",
    "regex_2 = \"message deleted by mumsnet.\"\n",
    "regex_3 = \"message deleted by mumsnet for breaking our talk guidelines.\"\n",
    "regex_4 = \"message withdrawn at poster's request.\"\n",
    "regex_5 = \"message withdrawn by mumsnet.\"\n",
    "\n",
    "\n",
    "regex_6 = '\\[deleted\\]'\n",
    "regex_7 = '\\[removed\\]'\n",
    "regex_8 = '(?i)I\\'m a bot'\n",
    "regex_9 = '(?i)I ?\\^?(\\'?m|am) \\^?\\^?a \\^?\\^?bot'\n",
    "regex_10 = '(?i)this bot wants to find'\n",
    "regex_11 = '\\[deleted by user\\]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "\n",
    "import re\n",
    "nan_value = float(\"NaN\")\n",
    "    \n",
    "def cleaning(text, regex):\n",
    "    proctext = str(text).lower()\n",
    "    if not (re.search(regex, proctext) is None):\n",
    "        return nan_value\n",
    "    else:\n",
    "        return proctext\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-destiny",
   "metadata": {},
   "source": [
    "Here we would apply one of the regular expression on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "\n",
    "sample['body'] = sample['body'].apply(lambda x: cleaning(x, regex_1))\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-landing",
   "metadata": {},
   "source": [
    "Now, we get rid of the rows with NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "\n",
    "sample.dropna(subset=['body'], inplace=True)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-melissa",
   "metadata": {},
   "source": [
    "And now we apply all the regular expressions we desinged on our data to clean all the unwanted posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "\n",
    "sample['body'] = sample['body'].apply(lambda x: cleaning(x, regex_2))\n",
    "sample.dropna(subset=['body'], inplace=True)\n",
    "sample['body'] = sample['body'].apply(lambda x: cleaning(x, regex_3))\n",
    "sample.dropna(subset=['body'], inplace=True)\n",
    "sample['body'] = sample['body'].apply(lambda x: cleaning(x, regex_4))\n",
    "sample.dropna(subset=['body'], inplace=True)\n",
    "sample['body'] = sample['body'].apply(lambda x: cleaning(x, regex_5))\n",
    "sample.dropna(subset=['body'], inplace=True)\n",
    "\n",
    "sample['author'] = sample['author'].apply(lambda x: cleaning(x, regex_6))\n",
    "sample.dropna(subset=['author'], inplace=True)\n",
    "sample['author'] = sample['author'].apply(lambda x: cleaning(x, regex_7))\n",
    "sample.dropna(subset=['author'], inplace=True)\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "\n",
    "# saving the cleaned data file\n",
    "sample.to_csv(path + file_name + '_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-scope",
   "metadata": {},
   "source": [
    "## Preparing the data for extracting the LIWC categorise\n",
    "\n",
    "In case you are dealing with large dataset (like in our case), we suggest you to save your dataset into a database tables. Having our data in format of a table gives us the following advantagous:\n",
    "\n",
    "- visualization (having access to different parts our data)\n",
    "- managable (updating our dataset)\n",
    "- searchable (searching for a specific user or post by mysql commands)\n",
    "\n",
    "You can follow the instructions in our other tutroal https://github.com/Identity-lab/Tutorial-on-salient-social-Identity-detection-model/blob/master/Tutorial_data_prepration.ipynb for this (because of time limitation we won't go through that in this tutorial).\n",
    "\n",
    "\n",
    "\n",
    "Next step is to prepare our data in a format compatible with the LWIC software. We do some extra cleaning by mainly dropping the new lines, several spaces, ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "def cleaning_for_LIWC(text, lowercase=True):\n",
    "    WHITESPACEREGEX = r'[ \\t\\n\\r\\f\\v]+'\n",
    "    NONPUNCREGEX = r'[a-zA-Z0-9_\\s]'\n",
    "    \n",
    "    if lowercase:\n",
    "        proctext = text.lower()\n",
    "    else:\n",
    "        proctext = text\n",
    "            \n",
    "    proctext = proctext.strip()\n",
    "    \n",
    "    # now replace newlines with space \n",
    "    proctext.replace('\\n',' ')\n",
    "\n",
    "    \n",
    "    # remove all duplicate whitespace\n",
    "    proctext = re.sub(WHITESPACEREGEX, ' ', proctext)\n",
    "    \n",
    "    # if text is only punctuation, remove it\n",
    "    if re.search(NONPUNCREGEX, proctext) is None:\n",
    "        proctext = ''\n",
    "        \n",
    "    return proctext\n",
    "\n",
    "\n",
    "\n",
    "def write_single_file(df, \n",
    "                      txtfpath, idfpath,  \n",
    "                      lowercase=True, parasep='\\n\\n\\n',\n",
    "                      dummytext='xxxdummyxxx'):\n",
    "\n",
    "    with open(txtfpath, 'w') as of_handler, open(idfpath, 'w') as idf_handler:\n",
    "        # iterate through and write each to file\n",
    "        # cursor.execute(selectquery)\n",
    "        count = 0\n",
    "        for i, row in df.iterrows():\n",
    "            id = row['id']\n",
    "            text = row['body']\n",
    "\n",
    "            # if the text does not contain any words then skip it\n",
    "            if re.search('[a-zA-Z]', text) is None:\n",
    "                continue\n",
    "\n",
    "            text = cleaning_for_LIWC(text,lowercase)\n",
    "\n",
    "            if len(text) == 0:\n",
    "                continue\n",
    "\n",
    "            # insert paragraph separator if line is greater than 0\n",
    "            if count > 0:\n",
    "                of_handler.write(parasep)\n",
    "                \n",
    "            text = str(text)\n",
    "                \n",
    "            # adding a dummy word at the enf of each post\n",
    "            # without it, the liwc results might be inconsistant with the data\n",
    "            of_handler.write(text + ' ' + dummytext)  \n",
    "            \n",
    "\n",
    "            idf_handler.write(str(id) + '\\n')\n",
    "            count += 1\n",
    "            if (i % 10000) == 0:\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "    if not test_numlines_txt_vs_ids(txtfpath, idfpath):\n",
    "                raise ValueError('File lines do not match')\n",
    "                \n",
    "\n",
    "def test_numlines_txt_vs_ids(txtfpath, idfpath):\n",
    "    numlines_txt = sum(1 for line in open(txtfpath))\n",
    "    numlines_ids = sum(1 for line in open(idfpath))\n",
    "\n",
    "    lengths_match = ((numlines_ids * 3) - 2) == numlines_txt\n",
    "\n",
    "\n",
    "    if lengths_match:\n",
    "        print('File lengths are consistent, with %d items' % (numlines_ids,))\n",
    "        return True\n",
    "    else:\n",
    "        print('File lengths are inconsistent')\n",
    "        print('numlines: txt=%d, ids=%d' % (numlines_txt, numlines_ids))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "\n",
    "filename = 'minimal'\n",
    "txtfpath = path + filename + '.txt'\n",
    "idfpath = path + filename + '.ids'\n",
    "\n",
    "write_single_file(sample,txtfpath, idfpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-duplicate",
   "metadata": {},
   "source": [
    "## Processing for LIWC features\n",
    "\n",
    "Now the data is ready to be imported to LIWC software. This requires a copy of the LIWC software, and a copy of LIWC dictionary (we used LIWC_2007 dictionary). Next step is to feed our text file into LIWC and extract the categories. Save the results in the same folder as the text and ids files (for example with name such as liwc_sample.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11\n",
    "\n",
    "liwcsample = pd.read_csv(path+'liwc_sample.csv')\n",
    "list(liwcsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12\n",
    "\n",
    "liwcsample = liwcsample.drop(['Filename', 'Segment'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13\n",
    "\n",
    "idsfile = open(path+\"minimal.ids\",\"r\")\n",
    "ids = idsfile.readlines()\n",
    "ids = [int(id.replace('\\n', '')) for id in ids]\n",
    "liwcsample['id'] = ids\n",
    "\n",
    "liwcsample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
