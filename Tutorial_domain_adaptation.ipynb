{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lightweight-blade",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In classification, it is typically assumed that the labeled training data comes from the same distribution as that of the test data. However, many real-world applications challenge this assumption.  In this context, the learner\n",
    "must take special care during the learning process to infer models that adapt well to the test data they are deployed on [1].\n",
    "\n",
    "\n",
    "These different but related marginal distributions are referred as domains.  In order to build robust classifiers, it is necessary to take into account the shift between these two distributions.[1]. \n",
    "\n",
    "\n",
    "##### Reference\n",
    "[1] Fernando, B., Habrard, A., Sebban, M., & Tuytelaars, T. (2013). Unsupervised visual domain adaptation using subspace alignment. In Proceedings of the IEEE international conference on computer vision (pp. 2960-2967).\n",
    "\n",
    "<br>\n",
    "\n",
    "## Applying the model to data from a different domain\n",
    "\n",
    "Here, we want to apply our identity detection model on data from Reddit, to identify the feminist vs parent posts. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "##### first, preparing the test samples from reddit data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "DBDIR = './data/'\n",
    "SAVEDIR = './save_dir/'\n",
    "model_name = 'logr.sav'\n",
    "\n",
    "Mumsnet_DB = 'Mumsnet_feminist_parent.csv'\n",
    "Reddit_DB = 'Reddit_feminist_parent.csv'\n",
    "\n",
    "\n",
    "# all LIWC stylistic features\n",
    "ALL_STYLISTIC_FEATURES = ['WPS', 'i', 'we', 'you', 'shehe', 'they', 'ipron','article', 'auxverb', 'past',\n",
    "                    'present', 'future', 'adverb', 'preps','conj', 'quant', 'number', 'time', 'Sixltr',\n",
    "                    'Period', 'Colon', 'SemiC', 'QMark', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP',\n",
    "                    'negate', 'swear', 'posemo','negemo', 'assent', 'nonfl', 'filler', 'Exclam', 'insight',\n",
    "                    'cause', 'discrep', 'tentat', 'certain', 'inhib', 'incl', 'excl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocessing(df, min_WC):\n",
    "    df = df.dropna()\n",
    "    df = df.loc[df['WC'] >= min_WC]\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_csv(path, min_WC = 25):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        df = preprocessing(df, min_WC)\n",
    "    except:\n",
    "        print('error in reading file')\n",
    "        raise\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "\n",
    "reddit_df = read_csv(DBDIR+Reddit_DB)\n",
    "reddit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "\n",
    "def separating_users(df):\n",
    "    fem_df = df.loc[df.forum_id == 1]\n",
    "    par_df = df.loc[df.forum_id == 0]\n",
    "    \n",
    "    # participants who are posting in both forums\n",
    "    within_p = set(fem_df.user_id.unique()).intersection(par_df.user_id.unique())\n",
    "    # participants who are posting only in one forum, parent or feminist\n",
    "    between_p = df[~df.user_id.isin(within_p)].user_id.unique()\n",
    "\n",
    "    return between_p, within_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "\n",
    "reddit_between_p, reddit_within_p = separating_users(reddit_df)\n",
    "print('number of between participants:%s'%len(reddit_between_p))\n",
    "print('number of within participants:%s'%len(reddit_within_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "\n",
    "def extract_testcases(posts_within, no=None):\n",
    "    # randomly selecting one post per forum for each within participant\n",
    "    testDB = posts_within.sample(frac=1, random_state=1)\n",
    "    testDB = testDB.drop_duplicates(subset=['user_id', 'forum_id'])\n",
    "\n",
    "    # if there is a limit on the number of test cases, we extract the test cases from \n",
    "    # no randomly choosen within participants\n",
    "    if no is not None:\n",
    "        within_participants = posts_within.user_id.unique()\n",
    "        testUsers = np.random.choice(within_participants, no, replace=False)\n",
    "        testDB = testDB.loc[testDB['user_id'].isin(testUsers)]\n",
    "\n",
    "    return testDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "\n",
    "reddit_within_posts = reddit_df.loc[reddit_df.user_id.isin(reddit_within_p)]\n",
    "\n",
    "reddit_testDB = extract_testcases(reddit_within_posts)\n",
    "reddit_testDB = reddit_testDB.reset_index(drop=True)\n",
    "reddit_testDB.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "\n",
    "reddit_X_test, reddit_y_test = reddit_testDB[ALL_STYLISTIC_FEATURES], reddit_testDB['forum_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-singapore",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### second, loading the identity detection model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "\n",
    "import joblib\n",
    "model = joblib.load(SAVEDIR + model_name)\n",
    "print('model is loaded ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "def test(model, X_test, y_test, verbose=False):\n",
    "    y_pred = model.predict(X_test)\n",
    "    ts_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    s = model.decision_function(X_test)\n",
    "    ts_auc = roc_auc_score(y_test, s)\n",
    "    \n",
    "    if verbose:\n",
    "        print('test accuracy:{}'.format(ts_acc), 'test AUC :{}'.format(ts_auc))\n",
    "    \n",
    "    return ts_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11\n",
    "\n",
    "_ = test(model, reddit_X_test, reddit_y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-landing",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "getting deeper into the results by checking the confusion matrix outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "predict_abs = model.predict(reddit_X_test)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(reddit_y_test, predict_abs).ravel()\n",
    "print('Out of %s test samples from parent forum, %s have been correctly predicted as parent post, \\n'\n",
    "      'and %s have been falsely predicted as feminist post.\\n'%(int(reddit_X_test.shape[0]/2), tn, fp))\n",
    "\n",
    "\n",
    "print('Out of %s test samples from feminist forum, %s have been correctly predicted as feminist post, \\n'\n",
    "      'and %s have been falsely predicted as parent post.'%(int(reddit_X_test.shape[0]/2), tp, fn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13\n",
    "\n",
    "plot_confusion_matrix(model, reddit_X_test, reddit_y_test) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14\n",
    "\n",
    "print('false positive error rate - rate of predicting a parent post as a feminist post:', fp / (fp + tn))\n",
    "print('false negative error rate - rate of predicting a feminist post as a parent post:', fn / (fn + tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-spider",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Confusion matrix results show the big difference between false-positive error rate and false-negative error rate. This shows that our model is more towards predicting a post as a feminist post than a parent post. \n",
    "\n",
    "This problem is probably because of the differences between the distribution of mumsnet data (used for training the model), and reddit data. To alleviate this problem, we apply a Domain Adaptation model on both training data and test data and re-train our model. \n",
    "\n",
    "<br>\n",
    "\n",
    "# Subspace Alignment\n",
    "In this tutorial, we apply an unsupervised DA solution proposed in [1], as subspace alignment, to learn a mapping function that aligns the source distribution with the target one, which in our case source data is mumsnet data and target data is reddit data (the unsupervised setting means that the training data only need the labeled examples from source data and unlabeled target examples).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-count",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "let's first prepare the source and target data (mumsnet and reddit data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15\n",
    "\n",
    "mumsnet_df = read_csv(DBDIR + Mumsnet_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16\n",
    "\n",
    "def get_train_set(df, batch_size, verbose=False):\n",
    "    df = df.sample(frac=1, random_state=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('number of between participants is:{}'.format(len(between_set.user_id.unique())))\n",
    "        \n",
    "    posts_fem = df[df['forum_id'] == 1]\n",
    "    posts_par = df[df['forum_id'] == 0]\n",
    "    \n",
    "    \n",
    "    if min(posts_fem.shape[0], posts_par.shape[0]) < batch_size :\n",
    "        batch_size = min(posts_fem.shape[0], posts_par.shape[0])\n",
    "\n",
    "    # buiding train set by randomly selecting posts from feminist and parent forums\n",
    "    posts_fem = df[df['forum_id'] == 1][:batch_size]\n",
    "    posts_par = df[df['forum_id'] == 0][:batch_size]\n",
    "    \n",
    "    trainDB = pd.concat([posts_fem, posts_par])\n",
    "    \n",
    "    \n",
    "    return trainDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17\n",
    "\n",
    "mumsnet_trainDB = get_train_set(mumsnet_df, batch_size=50000)\n",
    "mumsnet_trainDB = mumsnet_trainDB.reset_index(drop=True)\n",
    "print('\\ntrain set size:{}'.format(mumsnet_trainDB.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18\n",
    "\n",
    "mumsnet_df_rest = mumsnet_df.loc[~mumsnet_df.msg_id.isin(mumsnet_trainDB.msg_id)].copy(deep=True)\n",
    "mumsnet_between_p, mumsnet_within_p = separating_users(mumsnet_df_rest)\n",
    "mumsnet_within_posts = mumsnet_df_rest.loc[mumsnet_df_rest.user_id.isin(mumsnet_within_p)]\n",
    "mumsnet_testDB = extract_testcases(mumsnet_within_posts)\n",
    "mumsnet_testDB = mumsnet_testDB.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(mumsnet_trainDB.shape)\n",
    "print(mumsnet_testDB.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-passenger",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "we usually get the train instances as a random samples from all data, however, as the number of within participants are quite low in reddit data, we don't want to loose more of them by choosing their posts in train data. So, here, we choose the reddit_trainDB only from between posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "#19\n",
    "\n",
    "reddit_between_posts = reddit_df.loc[reddit_df.user_id.isin(reddit_between_p)]\n",
    "reddit_trainDB = get_train_set(reddit_between_posts, batch_size=50000)\n",
    "reddit_trainDB = reddit_trainDB.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(reddit_trainDB.shape)\n",
    "print(reddit_testDB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20\n",
    "\n",
    "source_train_X, source_train_y = mumsnet_trainDB[ALL_STYLISTIC_FEATURES], mumsnet_trainDB['forum_id']\n",
    "ssource_test_X, source_test_y = mumsnet_testDB[ALL_STYLISTIC_FEATURES], mumsnet_testDB['forum_id']\n",
    "\n",
    "target_train_X, target_train_y = reddit_trainDB[ALL_STYLISTIC_FEATURES], reddit_trainDB['forum_id']\n",
    "target_test_X, target_test_y = reddit_testDB[ALL_STYLISTIC_FEATURES], reddit_testDB['forum_id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-realtor",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Tuning the efficient number of eigenvectors (d_optimum)\n",
    "\n",
    "Even though both the source and target data (mumsnet and reddit data) lie in the same ùê∑-dimensional space (44 LIWC stylistic features), they have been drawn according to different distributions. Consequently, rather than working on the original data themselves, we learn the shift between these two domains. The goal is to shift both the source and target domain into a shared d-dimensional space. But, how do we calculate this optimum d-dimensional space?\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "We go through this process step-by-step: \n",
    "\n",
    "1- First, we transform source and target data in the form of a D-dimensional z-normalized vector (i.e. of zero mean and unit standard deviation), and then, using PCA, for each domain (mumsnet and reddit data), D eigenvectors which are ordered based on the eigenvalues.\n",
    "\n",
    "2- Second, we calculate the upper bound of the dimensionality (d-max) of the shared space.\n",
    "\n",
    "3- Third, Afterwards, we consider the subspaces of dimensionality from d = 1 to d_max and select the\n",
    "best d (d_optimum) that minimizes the classification error using a 10 fold cross-validation over the labeled source data (mumsnet data). \n",
    "\n",
    "4- Finally, we shift both source and target data into the shared subspace with the d-optimum dimensions.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "###### step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#21\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from numpy import linalg as LA\n",
    "import copy\n",
    "import math\n",
    "\n",
    "\n",
    "# using Principal Component Analysis (PCA) to identify eigenvectors and eigenvalues\n",
    "def PrincipleC(X, n):\n",
    "    C = np.dot(np.matrix(X).transpose(), np.matrix(X) ) /X.shape[1]\n",
    "    Lambda, U = LA.eig(C)\n",
    "    indices = np.argsort(Lambda)[::-1]\n",
    "    Lambda = [Lambda[i] for i in indices]\n",
    "    U = U[:, indices]\n",
    "    return U[: ,:n], Lambda[:n]\n",
    "\n",
    "\n",
    "\n",
    "# transforming the source and target data in the form of a D-dimensional z-normalized vector\n",
    "# and calling the PrincipleC\n",
    "def get_eigenvectors(X_source, X_target, dimensions):\n",
    "    #transform the data in the form of a D-dimensional z-normalized vector\n",
    "    X1 = stats.zscore(X_source)\n",
    "    X2 = stats.zscore(X_target)\n",
    "    \n",
    "    #extract the eigenvectors and eigenvalues\n",
    "    X_s, Lambda_s = PrincipleC(X1.copy(), dimensions)\n",
    "    X_t, Lambda_t = PrincipleC(X2.copy(), dimensions)\n",
    "    \n",
    "    \n",
    "    return X_s, X_t, Lambda_s, Lambda_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-quarterly",
   "metadata": {},
   "source": [
    "###### step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "#22\n",
    "\n",
    "\n",
    "#this function calculates the upper bound of d - maximum number of dimensions\n",
    "def get_max_dimension(X_source, X_target, dimensions, verbose=True):\n",
    "    # getting the eigenvectors and eigenvalues\n",
    "    X_s, X_t, Lambda_s, Lambda_t = get_eigenvectors(X_source, X_target, dimensions)\n",
    "\n",
    "    lambdas = []\n",
    "    gammas = []\n",
    "    B = 100 #a random positive number\n",
    "    delta = 0.1\n",
    "    n_min = np.minimum(X_source.shape[0], X_target.shape[0])\n",
    "    for i in range(0, dimensions-1):\n",
    "        lmin = np.minimum(Lambda_t[i]-Lambda_t[i+1], Lambda_s[i]-Lambda_s[i+1])\n",
    "        gamma = (1+np.sqrt(math.log(2/delta)/2))*((16*np.power(i+1, 3/2)*B)/(np.sqrt(n_min)*lmin))\n",
    "        lambdas.append({'d': i+1, 'lmin': copy.deepcopy(lmin)})\n",
    "        gammas.append(copy.deepcopy(gamma))\n",
    "\n",
    "    gamma = max(gammas)\n",
    "\n",
    "    d_max = 1\n",
    "    for dic_ in lambdas:\n",
    "        \n",
    "        d = dic_['d']\n",
    "        lmin = dic_['lmin']\n",
    "        upper_b = (1+np.sqrt(math.log(2/delta)/2))*((16*np.power(d, 3/2)*B)/(gamma*np.sqrt(n_min)))\n",
    "        if lmin >= upper_b:\n",
    "            d_max = d\n",
    "            \n",
    "    if verbose:\n",
    "        print('\\n upper dimension bound (d_max):', d_max)\n",
    "    return d_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "#23\n",
    "\n",
    "d_max = get_max_dimension(source_train_X[ALL_STYLISTIC_FEATURES].copy(deep=True), \n",
    "                          target_train_X[ALL_STYLISTIC_FEATURES].copy(deep=True), source_train_X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-output",
   "metadata": {},
   "source": [
    "by running the code above, we calculate the upper bound of dimensions for the new shared subspace, as d_max. In order to calculate the efficient number of dimensions, we run the function get_optimum_dimensions where for dimensionality from d = 1 to d_max, we shift source and target data, accordingly (function align), and calculate the accuracy of training and testing on source data (mumsnet data) using 10 fold cross-validation. The optimum dimensionality is the one that results in the highest accuracy on source data. \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "###### step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#24\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def Logistic_Regression(grid_search=False):\n",
    "    clf = LogisticRegression(solver='lbfgs', max_iter = 2000)\n",
    "\n",
    "    if grid_search:\n",
    "        tuned_parameters = [{'C': [1e-3, 1e-2, 1e-1, 1]}]\n",
    "        clf = GridSearchCV(LogisticRegression(solver='lbfgs'), tuned_parameters, cv=10,\n",
    "                           scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "def train(X_train, y_train, verbose=False):\n",
    "    model = Logistic_Regression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print('model is trained')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#25\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# the main function for shifting the source and target distributions\n",
    "def align(X_1, X_2, dim):\n",
    "    X_1 = stats.zscore(X_1)\n",
    "    X_2 = stats.zscore(X_2)\n",
    "\n",
    "    X_s, Lambda_s = PrincipleC(X_1.copy(), dim)\n",
    "    X_t, Lambda_t = PrincipleC(X_2.copy(), dim)\n",
    "\n",
    "    X_a = np.matrix(X_s) * np.matrix(X_s).transpose() * np.matrix(X_t)\n",
    "\n",
    "    S_a = np.matrix(X_1) * np.matrix(X_a)\n",
    "\n",
    "    T_t = np.matrix(X_2) * np.matrix(X_t)\n",
    "\n",
    "    return S_a, T_t\n",
    "\n",
    "\n",
    "# cross validation\n",
    "def test_cross_validation(df, label):\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    y = df[label].values\n",
    "    X = df.drop(columns=[label]).values\n",
    "    accuracy = []\n",
    "    for tr, te in cv.split(X, y):\n",
    "        clf = train(X[tr], y[tr])\n",
    "        acc_ = test(clf, X[te], y[te])\n",
    "        accuracy.append(copy.deepcopy(acc_))\n",
    "    return np.mean(accuracy)\n",
    "\n",
    "\n",
    "# calculating the optimum number of dimensions\n",
    "def get_optimum_dimension(d_max, X_source, y_source, X_target, label='forum_id', verbose=False):\n",
    "    acc_max = 0\n",
    "    d_optimum = 1\n",
    "    for j in range(1, d_max + 1):\n",
    "        X_s, X_t = align(copy.deepcopy(X_source), copy.deepcopy(X_target), j)\n",
    "        train_df = pd.concat([pd.DataFrame(X_s), y_source], axis=1)\n",
    "        acc_ = test_cross_validation(train_df, label=label)\n",
    "        if acc_ >= acc_max:\n",
    "            acc_max = acc_\n",
    "            d_optimum = j\n",
    "    if verbose:\n",
    "        print('\\n maximum accuracy for training and testing on source data:', acc_max)\n",
    "        print('\\n optimum dimensions:', d_optimum)\n",
    "\n",
    "    return d_optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "#26\n",
    "\n",
    "# calculate the optimum dimensionality\n",
    "d_optimum = get_optimum_dimension(d_max, source_train_X[ALL_STYLISTIC_FEATURES].copy(deep=True), source_train_y, \n",
    "                                  target_train_X[ALL_STYLISTIC_FEATURES].copy(deep=True), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-proof",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now that we calculated the optimum dimensionality, we can transform both source and target (mumsnet and reddit) data into the shared subspace:\n",
    "\n",
    "\n",
    "###### step4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#27\n",
    "\n",
    "def get_shared_subspace(X_1, X_2, d_optimum):\n",
    "    scaler_1 = StandardScaler().fit(X_1)\n",
    "    X_1 = scaler_1.transform(X_1)\n",
    "\n",
    "    scaler_2 = StandardScaler().fit(X_2)\n",
    "    X_2 = scaler_2.transform(X_2)\n",
    "\n",
    "    X_s, Lambda_s = PrincipleC(X_1.copy(), d_optimum)\n",
    "    X_t, Lambda_t = PrincipleC(X_2.copy(), d_optimum)\n",
    "\n",
    "    X_a = np.matrix(X_s) * np.matrix(X_s).transpose() * np.matrix(X_t)\n",
    "\n",
    "    return X_a, X_t, scaler_1, scaler_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "#28\n",
    "\n",
    "# create the shared subspace according to the optimum number of dimensions\n",
    "X_a, X_t, scaler_a, scaler_t = get_shared_subspace(source_train_X.copy(deep=True),target_train_X.copy(deep=True), \n",
    "                                                   d_optimum)\n",
    "\n",
    "# transform the source training data\n",
    "X_1 = scaler_a.transform(source_train_X)\n",
    "t_source_train_X = np.matrix(X_1) * np.matrix(X_a)\n",
    "\n",
    "\n",
    "# transform the target test data\n",
    "X_2 = scaler_t.transform(target_test_X)\n",
    "t_target_test_X = np.matrix(X_2) * np.matrix(X_t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-average",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "### Training and testing on the transformed data\n",
    "With both source training data and target test data are now transformed into a shared subspace, we re-train our model using the transformed mumsnet training data, and apply it on transformed reddit test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "#29\n",
    "\n",
    "# train a model using transformed source train data\n",
    "print('\\n training a model on transformed source data ...')\n",
    "retrained_model = train(pd.DataFrame(t_source_train_X), source_train_y)\n",
    "\n",
    "print('\\n test the model on transformed target test data: \\n')\n",
    "# test the model on transformed target test data\n",
    "c = test(retrained_model, pd.DataFrame(t_target_test_X), target_test_y, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-terrace",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "As you can see, the AUC hasn't changed much, however, the accuracy has improved substantially. Let's study the results by taking a look at the confusion matrix results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-copyright",
   "metadata": {},
   "outputs": [],
   "source": [
    "#30\n",
    "\n",
    "predict_abs = retrained_model.predict(pd.DataFrame(t_target_test_X))\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(target_test_y, predict_abs).ravel()\n",
    "print('false positive error rate - rate of predicting a parent post as a feminist post:', fp / (fp + tn))\n",
    "print('false negative error rate - rate of predicting a feminist post as a parent post:', fn / (fn + tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#31\n",
    "\n",
    "plot_confusion_matrix(retrained_model, t_target_test_X, target_test_y) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-ending",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As you can see, the error rates are now more balanced towards both feminist and parent posts, which was the objective of this part of the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
