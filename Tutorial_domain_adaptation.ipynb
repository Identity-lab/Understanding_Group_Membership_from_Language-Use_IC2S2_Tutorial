{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lightweight-blade",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In classification, it is typically assumed that the labeled training data comes from the same distribution as that of the test data. However, many real world applications challenge this assumption.  In this context, the learner\n",
    "must take special care during the learning process to infer models that adapt well to the test data they are deployed on [1].\n",
    "\n",
    "\n",
    "These different but related marginal distributions are referred as domains.  In order to build robust classifiers, it is necessary to take into account the shift between these two distributions.[1]. \n",
    "\n",
    "\n",
    "##### Reference\n",
    "[1] Fernando, B., Habrard, A., Sebban, M., & Tuytelaars, T. (2013). Unsupervised visual domain adaptation using subspace alignment. In Proceedings of the IEEE international conference on computer vision (pp. 2960-2967).\n",
    "\n",
    "<br>\n",
    "\n",
    "## Applying the model on data from different domain\n",
    "\n",
    "Here, we want to apply our identity detection model on data from Reddit, to identify the feminist vs parent posts. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "##### first, preparing the test samples from reddit data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "split-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "DBDIR = './data/'\n",
    "SAVEDIR = './save_dir/'\n",
    "model_name = 'logr.sav'\n",
    "\n",
    "Mumsnet_DB = 'Mumsnet_feminist_parent.csv'\n",
    "Reddit_DB = 'Reddit_feminist_parent.csv'\n",
    "\n",
    "\n",
    "# all LIWC stylistic features\n",
    "ALL_STYLISTIC_FEATURES = ['WPS', 'i', 'we', 'you', 'shehe', 'they', 'ipron','article', 'auxverb', 'past',\n",
    "                    'present', 'future', 'adverb', 'preps','conj', 'quant', 'number', 'time', 'Sixltr',\n",
    "                    'Period', 'Colon', 'SemiC', 'QMark', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP',\n",
    "                    'negate', 'swear', 'posemo','negemo', 'assent', 'nonfl', 'filler', 'Exclam', 'insight',\n",
    "                    'cause', 'discrep', 'tentat', 'certain', 'inhib', 'incl', 'excl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fifteen-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocessing(df, min_WC):\n",
    "    df = df.dropna()\n",
    "    df = df.loc[df['WC'] >= min_WC]\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_csv(path, min_WC = 25):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        df = preprocessing(df, min_WC)\n",
    "    except:\n",
    "        print('error in reading file')\n",
    "        raise\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "amino-nancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388096, 83)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df = read_csv(DBDIR+Reddit_DB)\n",
    "reddit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aerial-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separating_users(df):\n",
    "    fem_df = df.loc[df.forum_id == 1]\n",
    "    par_df = df.loc[df.forum_id == 0]\n",
    "    \n",
    "    # participants who are posting in both forums\n",
    "    within_p = set(fem_df.user_id.unique()).intersection(par_df.user_id.unique())\n",
    "    # participants who are posting only in one forum, parent or feminist\n",
    "    between_p = df[~df.user_id.isin(within_p)].user_id.unique()\n",
    "\n",
    "    return between_p, within_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "combined-being",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of between participants:49640\n",
      "number of within participants:263\n"
     ]
    }
   ],
   "source": [
    "reddit_between_p, reddit_within_p = separating_users(reddit_df)\n",
    "print('number of between participants:%s'%len(reddit_between_p))\n",
    "print('number of within participants:%s'%len(reddit_within_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "continuing-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_testcases(posts_within, no=None):\n",
    "    # randomly selecting one post per forum for each within participant\n",
    "    testDB = posts_within.sample(frac=1)\n",
    "    testDB = testDB.drop_duplicates(subset=['user_id', 'forum_id'])\n",
    "\n",
    "    # if there is a limit on the number of test cases, we extract the test cases from \n",
    "    # no randomly choosen within participants\n",
    "    if no is not None:\n",
    "        within_participants = posts_within.user_id.unique()\n",
    "        testUsers = np.random.choice(within_participants, no, replace=False)\n",
    "        testDB = testDB.loc[testDB['user_id'].isin(testUsers)]\n",
    "\n",
    "    return testDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "right-finance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(526, 83)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_within_posts = reddit_df.loc[reddit_df.user_id.isin(reddit_within_p)]\n",
    "\n",
    "reddit_testDB = extract_testcases(reddit_within_posts)\n",
    "reddit_testDB.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "varied-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_X_test, reddit_y_test = reddit_testDB[ALL_STYLISTIC_FEATURES], reddit_testDB['forum_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-singapore",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### second, loading the identity detection model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "oriented-leone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is loaded ...\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "model = joblib.load(SAVEDIR + model_name)\n",
    "print('model is loaded ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "phantom-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "def test(model, X_test, y_test, verbose=False):\n",
    "    y_pred = model.predict(X_test)\n",
    "    ts_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    s = model.decision_function(X_test)\n",
    "    ts_auc = roc_auc_score(y_test, s)\n",
    "    \n",
    "    if verbose:\n",
    "        print('test accuracy:{}'.format(ts_acc), 'test AUC :{}'.format(ts_auc))\n",
    "    \n",
    "    return ts_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "authentic-graphic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:0.6273764258555133 test AUC :0.7169685841923406\n"
     ]
    }
   ],
   "source": [
    "_ = test(model, reddit_X_test, reddit_y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-landing",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "getting deeper into the results by checking the confusion matrix outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pleasant-audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 263 test samples from parent forum, 114 have been correctly predicted as parent post, \n",
      "and 149 have been falsely predicted as feminist post.\n",
      "\n",
      "Out of 263 test samples from feminist forum, 216 have been correctly predicted as feminist post, \n",
      "and 47 have been falsely predicted as parent post.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "predict_abs = model.predict(reddit_X_test)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(reddit_y_test, predict_abs).ravel()\n",
    "print('Out of %s test samples from parent forum, %s have been correctly predicted as parent post, \\n'\n",
    "      'and %s have been falsely predicted as feminist post.\\n'%(int(reddit_X_test.shape[0]/2), tn, fp))\n",
    "\n",
    "\n",
    "print('Out of %s test samples from feminist forum, %s have been correctly predicted as feminist post, \\n'\n",
    "      'and %s have been falsely predicted as parent post.'%(int(reddit_X_test.shape[0]/2), tp, fn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "shared-listening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(model, reddit_X_test, reddit_y_test) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "frozen-explorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive error rate - rate of predicting a parent post as a feminist post: 0.5665399239543726\n",
      "false negative error rate - rate of predicting a feminist post as a parent post: 0.17870722433460076\n"
     ]
    }
   ],
   "source": [
    "print('false positive error rate - rate of predicting a parent post as a feminist post:', fp / (fp + tn))\n",
    "print('false negative error rate - rate of predicting a feminist post as a parent post:', fn / (fn + tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-spider",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Te confision matrix results shows the big difference between false positive error rate and false negative error rate. This shows that our model is more towards predicting a post as a feminist post than a parent post. \n",
    "\n",
    "This problem is probably because of the differences between the distribution of mumsnet data (used for training the model), and reddit data. To aleviate this problem, we apply a Domain Adaptation model on both trainng data and test data, and re-train our model. \n",
    "\n",
    "<br>\n",
    "\n",
    "# Subspace Alignment\n",
    "In this tutorial, we apply an unsupervised DA solution proposed in [1], as subspace alignment, to learn a mapping function which aligns the source distribution with the target one, which in our case source data is mumsnet data and target data is reddit data (the unsupervised setting means that the training data only need the labelled examples from source data and unlabeled target examples).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-count",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "let's first prepare the source and target data (mumsnet and reddit data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "going-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "mumsnet_df = read_csv(DBDIR + Mumsnet_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "framed-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_set(df, batch_size, verbose=False):\n",
    "    df = df.sample(frac=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('number of between participants is:{}'.format(len(between_set.user_id.unique())))\n",
    "        \n",
    "    posts_fem = df[df['forum_id'] == 1]\n",
    "    posts_par = df[df['forum_id'] == 0]\n",
    "    \n",
    "    \n",
    "    if min(posts_fem.shape[0], posts_par.shape[0]) < batch_size :\n",
    "        batch_size = min(posts_fem.shape[0], posts_par.shape[0])\n",
    "\n",
    "    # buiding train set by randomly selecting posts from feminist and parent forums\n",
    "    posts_fem = df[df['forum_id'] == 1][:batch_size]\n",
    "    posts_par = df[df['forum_id'] == 0][:batch_size]\n",
    "    \n",
    "    trainDB = pd.concat([posts_fem, posts_par])\n",
    "    \n",
    "    \n",
    "    return trainDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "capable-turkey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train set size:100000\n"
     ]
    }
   ],
   "source": [
    "mumsnet_trainDB = get_train_set(mumsnet_df, batch_size=50000)\n",
    "print('\\ntrain set size:{}'.format(mumsnet_trainDB.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sacred-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "mumsnet_df_rest = mumsnet_df.loc[~mumsnet_df.msg_id.isin(mumsnet_trainDB.msg_id)].copy(deep=True)\n",
    "mumsnet_between_p, mumsnet_within_p = separating_users(mumsnet_df_rest)\n",
    "mumsnet_within_posts = mumsnet_df_rest.loc[mumsnet_df_rest.user_id.isin(mumsnet_within_p)]\n",
    "mumsnet_testDB = extract_testcases(mumsnet_within_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "subtle-gateway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 84)\n",
      "(4002, 84)\n"
     ]
    }
   ],
   "source": [
    "mumsnet_trainDB = mumsnet_trainDB.reset_index(drop=True)\n",
    "mumsnet_testDB = mumsnet_testDB.reset_index(drop=True)\n",
    "\n",
    "print(mumsnet_trainDB.shape)\n",
    "print(mumsnet_testDB.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-passenger",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "we usually get the train instances as a random samples from all data, however, as the number of within participants are quite low in reddit data, we don't want to loose more of them by choosing their posts in train data. So, here, we choose the reddit_trainDB only from between posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "diagnostic-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_between_posts = reddit_df.loc[reddit_df.user_id.isin(reddit_between_p)]\n",
    "reddit_trainDB = get_train_set(reddit_between_posts, batch_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "chief-label",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51366, 83)\n",
      "(526, 83)\n"
     ]
    }
   ],
   "source": [
    "reddit_trainDB = reddit_trainDB.reset_index(drop=True)\n",
    "reddit_testDB = reddit_testDB.reset_index(drop=True)\n",
    "\n",
    "print(reddit_trainDB.shape)\n",
    "print(reddit_testDB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "champion-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train_X, source_train_y = mumsnet_trainDB[ALL_STYLISTIC_FEATURES], mumsnet_trainDB['forum_id']\n",
    "ssource_test_X, source_test_y = mumsnet_testDB[ALL_STYLISTIC_FEATURES], mumsnet_testDB['forum_id']\n",
    "\n",
    "target_train_X, target_train_y = reddit_trainDB[ALL_STYLISTIC_FEATURES], reddit_trainDB['forum_id']\n",
    "target_test_X, target_test_y = reddit_testDB[ALL_STYLISTIC_FEATURES], reddit_testDB['forum_id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-realtor",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Tuning the efficient number of eigenvectors (d_optimum)\n",
    "\n",
    "Even though both the source and target data (mumsnet and reddit data) lie in the same 𝐷-dimensional space (44 LIWC stylistic features), they have been drawn according to different distributions. Consequently, rather than working on the original data themselves, we learn the shift between these two domains. The goal is to shift both the source and target domain into a shared d-dimensional space. But, how we calculate this optimum d-dimensional space?\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "We go through this process step-by-step: \n",
    "\n",
    "1- First, we transform source and target data in the form of a D-dimensional z-normalized vector (i.e. of zero mean and unit standard deviation), and then, using PCA, for each domain (mumsnet and ressit data), D eigenvectors which are ordered based on the eigenvalues.\n",
    "\n",
    "2- Second, we calculate the upper bound of the dimensionality (d-max) of the shared space.\n",
    "\n",
    "3- Third, Afterwards, we consider the subspaces of dimensionality from d = 1 to d_max and select the\n",
    "best d (d_optimum) that minimizes the classification error using a 10 fold cross-validation over the labelled source data (mumsnet data). \n",
    "\n",
    "4- Finally, we shift both source and target data into the shared subspace with the d-optimum dimensions.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "###### step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "through-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from numpy import linalg as LA\n",
    "import copy\n",
    "import math\n",
    "\n",
    "\n",
    "# using Principal Component Analysis (PCA) to identify eigenvectors and eigenvalues\n",
    "def PrincipleC(X, n):\n",
    "    C = np.dot(np.matrix(X).transpose(), np.matrix(X) ) /X.shape[1]\n",
    "    Lambda, U = LA.eig(C)\n",
    "    indices = np.argsort(Lambda)[::-1]\n",
    "    Lambda = [Lambda[i] for i in indices]\n",
    "    U = U[:, indices]\n",
    "    return U[: ,:n], Lambda[:n]\n",
    "\n",
    "\n",
    "\n",
    "# transforming the source and target data in the form of a D-dimensional z-normalized vector\n",
    "# and calling the PrincipleC\n",
    "def get_eigenvectors(X_source, X_target, dimensions):\n",
    "    #transform the data in the form of a D-dimensional z-normalized vector\n",
    "    X1 = stats.zscore(X_source)\n",
    "    X2 = stats.zscore(X_target)\n",
    "    \n",
    "    #extract the eigenvectors and eigenvalues\n",
    "    X_s, Lambda_s = PrincipleC(X1.copy(), dimensions)\n",
    "    X_t, Lambda_t = PrincipleC(X2.copy(), dimensions)\n",
    "    \n",
    "    \n",
    "    return X_s, X_t, Lambda_s, Lambda_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-quarterly",
   "metadata": {},
   "source": [
    "###### step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ancient-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function calculates the upper bound of d - maximum number of dimensions\n",
    "def get_max_dimension(X_source, X_target, dimensions, verbose=True):\n",
    "    # getting the eigenvectors and eigenvalues\n",
    "    X_s, X_t, Lambda_s, Lambda_t = get_eigenvectors(X_source, X_target, dimensions)\n",
    "\n",
    "    lambdas = []\n",
    "    gammas = []\n",
    "    B = 100 #a random positive number\n",
    "    delta = 0.1\n",
    "    n_min = np.minimum(X_source.shape[0], X_target.shape[0])\n",
    "    for i in range(0, dimensions-1):\n",
    "        lmin = np.minimum(Lambda_t[i]-Lambda_t[i+1], Lambda_s[i]-Lambda_s[i+1])\n",
    "        gamma = (1+np.sqrt(math.log(2/delta)/2))*((16*np.power(i+1, 3/2)*B)/(np.sqrt(n_min)*lmin))\n",
    "        lambdas.append({'d': i+1, 'lmin': copy.deepcopy(lmin)})\n",
    "        gammas.append(copy.deepcopy(gamma))\n",
    "\n",
    "    gamma = max(gammas)\n",
    "\n",
    "    d_max = 1\n",
    "    for dic_ in lambdas:\n",
    "        \n",
    "        d = dic_['d']\n",
    "        lmin = dic_['lmin']\n",
    "        upper_b = (1+np.sqrt(math.log(2/delta)/2))*((16*np.power(d, 3/2)*B)/(gamma*np.sqrt(n_min)))\n",
    "        if lmin >= upper_b:\n",
    "            d_max = d\n",
    "            \n",
    "    if verbose:\n",
    "        print('\\n upper dimension bound (d_max):', d_max)\n",
    "    return d_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "excessive-market",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " upper dimension bound (d_max): 43\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d_max = get_max_dimension(source_train_X[ALL_STYLISTIC_FEATURES].copy(deep=True), \n",
    "                          target_train_X[ALL_STYLISTIC_FEATURES].copy(deep=True), source_train_X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-output",
   "metadata": {},
   "source": [
    "by running the code above, we calculate the upper bound of d, as d_max. In order to calculate the efficient number of dimensions, we run the function get_optimum_dimensions where for dimensionality from d = 1 to d_max, we shiftf source and target data accordignly (function align) and calculate the accuracy of training and testing on source data (mumsnet data) using 10 fold cross validation. The optimum dimensionality is the one which results in the highest accuracy on source data. \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "###### step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "enabling-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def Logistic_Regression(grid_search=False):\n",
    "    clf = LogisticRegression(solver='lbfgs', max_iter = 2000)\n",
    "\n",
    "    if grid_search:\n",
    "        tuned_parameters = [{'C': [1e-3, 1e-2, 1e-1, 1]}]\n",
    "        clf = GridSearchCV(LogisticRegression(solver='lbfgs'), tuned_parameters, cv=10,\n",
    "                           scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "def train(X_train, y_train, verbose=False):\n",
    "    model = Logistic_Regression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print('model is trained')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "informative-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# the main function for shifting the source and target distributions\n",
    "def align(X_1, X_2, dim):\n",
    "    X_1 = stats.zscore(X_1)\n",
    "    X_2 = stats.zscore(X_2)\n",
    "\n",
    "    X_s, Lambda_s = PrincipleC(X_1.copy(), dim)\n",
    "    X_t, Lambda_t = PrincipleC(X_2.copy(), dim)\n",
    "\n",
    "    X_a = np.matrix(X_s) * np.matrix(X_s).transpose() * np.matrix(X_t)\n",
    "\n",
    "    S_a = np.matrix(X_1) * np.matrix(X_a)\n",
    "\n",
    "    T_t = np.matrix(X_2) * np.matrix(X_t)\n",
    "\n",
    "    return S_a, T_t\n",
    "\n",
    "\n",
    "# cross validation\n",
    "def test_cross_validation(df, label):\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    y = df[label].values\n",
    "    X = df.drop(columns=[label]).values\n",
    "    accuracy = []\n",
    "    for tr, te in cv.split(X, y):\n",
    "        clf = train(X[tr], y[tr])\n",
    "        acc_ = test(clf, X[te], y[te])\n",
    "        accuracy.append(copy.deepcopy(acc_))\n",
    "    return np.mean(accuracy)\n",
    "\n",
    "\n",
    "# calculating the optimum number of dimensions\n",
    "def get_optimum_dimension(d_max, X_source, y_source, X_target, label='forum_id', verbose=False):\n",
    "    acc_max = 0\n",
    "    d_optimum = 1\n",
    "    for j in range(1, d_max + 1):\n",
    "        X_s, X_t = align(copy.deepcopy(X_source), copy.deepcopy(X_target), j)\n",
    "        train_df = pd.concat([pd.DataFrame(X_s), y_source], axis=1)\n",
    "        acc_ = test_cross_validation(train_df, label=label)\n",
    "        if acc_ >= acc_max:\n",
    "            acc_max = acc_\n",
    "            d_optimum = j\n",
    "    if verbose:\n",
    "        print('\\n maximum accuracy for training and testing on source data:', acc_max)\n",
    "        print('\\n optimum dimensions:', d_optimum)\n",
    "\n",
    "    return d_optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "retained-elite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " maximum accuracy for training and testing on source data: 0.8447899999999999\n",
      "\n",
      " optimum dimensions: 43\n"
     ]
    }
   ],
   "source": [
    "# calculate the optimum dimensionality\n",
    "d_optimum = get_optimum_dimension(d_max, source_train_X[ALL_STYLISTIC_FEATURES].copy(deep=True), source_train_y, \n",
    "                                  target_train_X[ALL_STYLISTIC_FEATURES].copy(deep=True), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-proof",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now that we calculated the optimum dimensionality, we can transform both source and target (mumsnet and reddit) data in to the shared subspace:\n",
    "\n",
    "\n",
    "###### step4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "working-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shared_subspace(X_1, X_2, d_optimum):\n",
    "    scaler_1 = StandardScaler().fit(X_1)\n",
    "    X_1 = scaler_1.transform(X_1)\n",
    "\n",
    "    scaler_2 = StandardScaler().fit(X_2)\n",
    "    X_2 = scaler_2.transform(X_2)\n",
    "\n",
    "    X_s, Lambda_s = PrincipleC(X_1.copy(), d_optimum)\n",
    "    X_t, Lambda_t = PrincipleC(X_2.copy(), d_optimum)\n",
    "\n",
    "    X_a = np.matrix(X_s) * np.matrix(X_s).transpose() * np.matrix(X_t)\n",
    "\n",
    "    return X_a, X_t, scaler_1, scaler_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "liquid-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create the shared subspace according to the optimum number of dimensions\n",
    "X_a, X_t, scaler_a, scaler_t = get_shared_subspace(source_train_X.copy(deep=True),target_train_X.copy(deep=True), \n",
    "                                                   d_optimum)\n",
    "\n",
    "# transform the source training data\n",
    "X_1 = scaler_a.transform(source_train_X)\n",
    "t_source_train_X = np.matrix(X_1) * np.matrix(X_a)\n",
    "\n",
    "\n",
    "# transform the target test data\n",
    "X_2 = scaler_t.transform(target_test_X)\n",
    "t_target_test_X = np.matrix(X_2) * np.matrix(X_t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-average",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "### Training and testing on the transformed data\n",
    "With both source training data and target test data are now transofrmed into a shared subspace, we re-train our model using the transformed mumsnet training data, and apply it on trasformed reddit test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fresh-integral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " training a model on transformed source data ...\n",
      "\n",
      " test the model on transformed target test data: \n",
      "\n",
      "test accuracy:0.6901140684410646 test AUC :0.719296216513178\n"
     ]
    }
   ],
   "source": [
    "# train a model using transformed source train data\n",
    "print('\\n training a model on transformed source data ...')\n",
    "retrained_model = train(pd.DataFrame(t_source_train_X), source_train_y)\n",
    "\n",
    "print('\\n test the model on transformed target test data: \\n')\n",
    "# test the model on transformed target test data\n",
    "c = test(retrained_model, pd.DataFrame(t_target_test_X), target_test_y, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-terrace",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "As you can see, the AUC hasn't channged much, however, the accuracy has improved substantially. Let's study the results by taking a look at the confusion matrix results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "split-copyright",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive error rate - rate of predicting a parent post as a feminist post: 0.29277566539923955\n",
      "false negative error rate - rate of predicting a feminist post as a parent post: 0.3269961977186312\n"
     ]
    }
   ],
   "source": [
    "predict_abs = retrained_model.predict(pd.DataFrame(t_target_test_X))\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(target_test_y, predict_abs).ravel()\n",
    "print('false positive error rate - rate of predicting a parent post as a feminist post:', fp / (fp + tn))\n",
    "print('false negative error rate - rate of predicting a feminist post as a parent post:', fn / (fn + tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bearing-athletics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(retrained_model, t_target_test_X, target_test_y) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-ending",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As you can see, the error rates are now balanced towards both feminist and parent posts, which was the objective of this part of the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
