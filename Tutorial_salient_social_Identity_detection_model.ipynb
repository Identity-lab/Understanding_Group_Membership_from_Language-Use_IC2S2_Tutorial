{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minimal-antarctica",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "This is the python implementation of the paper \"ASIA: Automated Social Identity Assessment using linguistic style\".\n",
    "\n",
    "This tutorial is aimed at readers who have a basic familiarity with Python. You can find the original version of this tutorial at https://github.com/Identity-lab/Tutorial-on-salient-social-Identity-detection-model.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "In order to run the code, first you need to\n",
    "\n",
    "- download the datasets (Mumsnet_feminist_parent.csv, Reddit_feminist_parent.csv)\n",
    "- copy the datasets in a folder named data\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "Each of these csv files contains posts from both parent and feminist forums. \n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "## Reading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "# path to data\n",
    "DBDIR = './data/'\n",
    "# path to where the model would be saved\n",
    "SAVEDIR = './save_dir'\n",
    "\n",
    "# datasets\n",
    "Mumsnet_DB = 'Mumsnet_feminist_parent.csv'\n",
    "\n",
    "# all LIWC stylistic features\n",
    "ALL_STYLISTIC_FEATURES = ['WPS', 'i', 'we', 'you', 'shehe', 'they', 'ipron','article', 'auxverb', 'past',\n",
    "                    'present', 'future', 'adverb', 'preps','conj', 'quant', 'number', 'time', 'Sixltr',\n",
    "                    'Period', 'Colon', 'SemiC', 'QMark', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP',\n",
    "                    'negate', 'swear', 'posemo','negemo', 'assent', 'nonfl', 'filler', 'Exclam', 'insight',\n",
    "                    'cause', 'discrep', 'tentat', 'certain', 'inhib', 'incl', 'excl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-gilbert",
   "metadata": {},
   "source": [
    "Here we considered all the stylistic LIWC features for training our identity detection model, however, any subset of these features can be considered. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "First, we read our dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def read_csv(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except:\n",
    "        print('error in reading file')\n",
    "        raise\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "\n",
    "mumsnet_df = read_csv(DBDIR + Mumsnet_DB)\n",
    "print('mumsnet features:%s'%list(mumsnet_df))\n",
    "print(mumsnet_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-lightning",
   "metadata": {},
   "source": [
    "Then, we filter the short posts (posts with less than a specific word numbers) - this step is optional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "\n",
    "def preprocessing(df, min_WC):\n",
    "    df = df.dropna()\n",
    "    df = df.loc[df['WC'] >= min_WC]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "\n",
    "min_WC = 25\n",
    "\n",
    "mumsnet_df = preprocessing(mumsnet_df, min_WC)\n",
    "print(mumsnet_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-duncan",
   "metadata": {},
   "source": [
    "### Buiding the train set and the test set\n",
    "\n",
    "We train our model on posts from mumsnet data, and later on, we would apply our trained model on data from Reddit.\n",
    "\n",
    "<br>\n",
    "\n",
    "So, let's first build the train and test sets from mumsnet data. \n",
    "- The training set containes 100,000 posts randomly drawn from muments data, 50,000 posts from each forums, feminist and parent. We call that between samples. \n",
    "- Test set on the other hand, contains posts written by those who are contributing in both forums (one per each forum), we call that within samples. \n",
    "\n",
    "<br>\n",
    "\n",
    "For this, we first need to separate individuals who are posting in both forums from those who are contributing only in one of the forums (we call them between and within participants).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "\n",
    "def separating_users(df):\n",
    "    fem_df = df.loc[df.forum_id == 1]\n",
    "    par_df = df.loc[df.forum_id == 0]\n",
    "    \n",
    "    # participants who are posting in both forums\n",
    "    within_p = set(fem_df.user_id.unique()).intersection(par_df.user_id.unique())\n",
    "    # participants who are posting only in one forum, parent or feminist\n",
    "    between_p = df[~df.user_id.isin(within_p)].user_id.unique()\n",
    "\n",
    "    return between_p, within_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "\n",
    "between_p, within_p = separating_users(mumsnet_df)\n",
    "print('number of between participants:%s'%len(between_p))\n",
    "print('number of within participants:%s'%len(within_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-tattoo",
   "metadata": {},
   "source": [
    "<br>\n",
    "Well, as it is mentioned, we extract the training set from all the posts, 50000 from each forums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "\n",
    "def get_train_set(df, batch_size):\n",
    "    # shuffle the rows\n",
    "    df = df.sample(frac=1, random_state=1)\n",
    "\n",
    "    # buiding train set by randomly selecting posts from feminist and parent forums\n",
    "    posts_fem = df[df['forum_id'] == 1][:batch_size]\n",
    "    posts_par = df[df['forum_id'] == 0][:batch_size]\n",
    "    \n",
    "    trainDB = pd.concat([posts_fem, posts_par])\n",
    "    \n",
    "    return trainDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "\n",
    "trainDB = get_train_set(mumsnet_df, batch_size=50000)\n",
    "print('\\ntrain set size:{}'.format(trainDB.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-peripheral",
   "metadata": {},
   "source": [
    "<br>\n",
    "Test posts then would be sampled from within posts, but from those within participants whose their posts are not among the training posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "\n",
    "mumsnet_df_rest = mumsnet_df.loc[~mumsnet_df.msg_id.isin(trainDB.msg_id)].copy(deep=True)\n",
    "mumsnet_df_rest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11\n",
    "\n",
    "between_p, within_p = separating_users(mumsnet_df_rest)\n",
    "print('remaining number of between participants:%s'%len(between_p))\n",
    "print('remaining number of within participants:%s'%len(within_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12\n",
    "\n",
    "within_posts = mumsnet_df_rest.loc[mumsnet_df_rest.user_id.isin(within_p)]\n",
    "within_posts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-organ",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now we have the within posts, we randomly select one post per forum from each within participant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13\n",
    "\n",
    "def extract_testcases(posts_within, no=None):\n",
    "    # randomly selecting one post per forum for each within participant\n",
    "    testDB = posts_within.sample(frac=1, random_state=1)\n",
    "    testDB = testDB.drop_duplicates(subset=['user_id', 'forum_id'])\n",
    "\n",
    "    # if there is a limit on the number of test cases, we extract the test cases from \n",
    "    # no randomly choosen within participants\n",
    "    if no is not None:\n",
    "        within_participants = posts_within.user_id.unique()\n",
    "        testUsers = np.random.choice(within_participants, no, replace=False)\n",
    "        testDB = testDB.loc[testDB['user_id'].isin(testUsers)]\n",
    "\n",
    "    return testDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14\n",
    "\n",
    "testDB = extract_testcases(within_posts)\n",
    "testDB.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-latter",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "## Training our model\n",
    "\n",
    "\n",
    "\n",
    "Here we apply Logistic regression for training our model. If grid_search is true, best parameter would be chosen for the model, but it takes longer to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "def Logistic_Regression(grid_search=False):\n",
    "    # to add the regularization, you can set penalty=‘l1’ or penalty=‘l2’ (depending on the solver).\n",
    "    model = LogisticRegression(solver='lbfgs', max_iter = 2000, penalty='l2')\n",
    "    \n",
    "    \n",
    "    if grid_search:\n",
    "        tuned_parameters = [{'C': [1e-3, 1e-2, 1e-1, 1]}]\n",
    "        model = GridSearchCV(LogisticRegression(solver='lbfgs'), tuned_parameters, cv=10,\n",
    "                           scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(X_train, y_train, cross_val=True, save_dir=SAVEDIR):\n",
    "    \n",
    "    \n",
    "    model = Logistic_Regression()\n",
    "\n",
    "    # training accuracy and AUC\n",
    "    tr_auc, tr_acc = None, None\n",
    "    if cross_val:\n",
    "        tr_auc = np.mean(cross_val_score(model, X_train, y_train, cv=10, scoring='roc_auc'))\n",
    "        tr_acc = np.mean(cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy'))\n",
    "        \n",
    "        print('training accuracy:{}'.format(tr_acc), 'training AUC :{}'.format(tr_auc))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    joblib.dump(model, save_dir+'/logr.sav')\n",
    "\n",
    "    print('\\n our model is trained and saved in %s'%save_dir+'/logr.sav')\n",
    "\n",
    "    return model, tr_auc, tr_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16\n",
    "\n",
    "X_train, y_train = trainDB[ALL_STYLISTIC_FEATURES], trainDB['forum_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17\n",
    "\n",
    "model, tr_auc, tr_acc = train(X_train, y_train, cross_val=False, save_dir=SAVEDIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-julian",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "## Testing our model\n",
    "\n",
    "\n",
    "\n",
    "Via cross validation we get an estimate of how well our model is performing when it comes to the between posts.Now, in order to do the proper test, we apply ourtrained model on the within posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18\n",
    "\n",
    "def test(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    ts_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    s = model.decision_function(X_test)\n",
    "    ts_auc = roc_auc_score(y_test, s)\n",
    "\n",
    "    print('test accuracy:{}'.format(ts_acc), 'test AUC :{}'.format(ts_auc))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#19\n",
    "\n",
    "X_test, y_test = testDB[ALL_STYLISTIC_FEATURES], testDB['forum_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20\n",
    "\n",
    "test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-modification",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "Now we know how accurate our model is, let's get a little bit deeper into the results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#21\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "predict_abs = model.predict(X_test)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predict_abs).ravel()\n",
    "\n",
    "print('Out of %s test samples from parent forum, %s have been correctly predicted as parent post, \\n'\n",
    "      'and %s have been falsely predicted as feminist post.\\n'%(int(X_test.shape[0]/2), tn, fp))\n",
    "\n",
    "\n",
    "print('Out of %s test samples from feminist forum, %s have been correctly predicted as feminist post, \\n'\n",
    "      'and %s have been falsely predicted as parent post.'%(int(X_test.shape[0]/2), tp, fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "#22\n",
    "\n",
    "plot_confusion_matrix(model, X_test, y_test) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "#23\n",
    "\n",
    "print('false positive error rate - rate of predicting a parent post as a feminist post:', fp / (fp + tn))\n",
    "print('false negative error rate - rate of predicting a feminist post as a parent post:', fn / (fn + tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#24\n",
    "\n",
    "plot_roc_curve(model, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-bathroom",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### What does that mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "#25\n",
    "\n",
    "coefficients = dict(list(zip(ALL_STYLISTIC_FEATURES, model.coef_[0])))\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-tongue",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now, we want to see how important each stylistic LIWC feature is in identifying parent vs feminist identities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "#26\n",
    "\n",
    "Xtrain_std = X_train.std(axis=0, skipna=True)\n",
    "Xtrain_std = dict(list(zip(ALL_STYLISTIC_FEATURES, Xtrain_std)))\n",
    "\n",
    "feature_importance = {feature:coefficients[feature] * Xtrain_std[feature] for feature in ALL_STYLISTIC_FEATURES}\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "#27\n",
    "\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-1.2, 1.2])\n",
    "axes.set_position([0.06, 0.1999999999999999, 0.93, 0.77])\n",
    "\n",
    "x_pos = np.arange(len(ALL_STYLISTIC_FEATURES))\n",
    "\n",
    "for x, y in list(zip(x_pos, feature_importance.values())):\n",
    "    if y > 0:\n",
    "        axes.scatter(x, y, marker='v', c='blue', s=80)\n",
    "    else:\n",
    "        axes.scatter(x, y, marker='^', c='orange', s=80)\n",
    "plt.xticks(x_pos, ALL_STYLISTIC_FEATURES)\n",
    "orange_triangle = mlines.Line2D([], [], color='orange', marker='^', linestyle='None',\n",
    "                                markersize=10, label='Feminist')\n",
    "blue_triangle = mlines.Line2D([], [], color='blue', marker='v', linestyle='None',\n",
    "                              markersize=10, label='parent')\n",
    "\n",
    "axes.legend((orange_triangle, blue_triangle), ('Parent', 'Feminist'), loc='upper left')\n",
    "\n",
    "plt.axhline(y=0.0, color='r', linestyle='--')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "axes.tick_params(axis='x', pad=10)\n",
    "plt.yticks(rotation=90)\n",
    "for tick in axes.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(15)\n",
    "    tick.label1.set_fontweight('bold')\n",
    "for tick in axes.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(15)\n",
    "    tick.label1.set_fontweight('bold')\n",
    "plt.ylabel('Feature Importance', fontsize=22, labelpad=20, fontweight='bold')\n",
    "\n",
    "plt.grid(b=True, axis='x', color='gray', linestyle='-', linewidth=0.1)\n",
    "plt.savefig(SAVEDIR+'/featureimportance.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
